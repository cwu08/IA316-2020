{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jan 23  IA316  TP  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User : $u\\in \\mathbb{R}^{k\\cdot \\#user}$\n",
    "\n",
    "Item : $i\\in \\mathbb{R}^{k\\cdot \\#item}$ \n",
    "\n",
    "Reward : $r \\in \\{1,2,3,4,5\\}, r = \\frac{<u,i>}{||u||\\times||i||} = f(u,i)$\n",
    "\n",
    "$k$ embedding space dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating environment, items, rewards\n",
    "# A client comes. The environment will return a list of items\n",
    "# return [user_id, [list_item_id]]\n",
    "# then an agent receives the item_id list and it does an act. \n",
    "\n",
    "# 记录推荐历史，推荐过的物品就不再推荐\n",
    "\n",
    "class Environment:\n",
    "    \"\"\"\n",
    "    rating environment, users, items, rewards\n",
    "    \n",
    "    A client comes. The environment will return a list of items\n",
    "    return [user_id, [list_item_id]]\n",
    "    rewards = [1,2,3,4,5]\n",
    "    \"\"\"\n",
    "    def __init__(self, k, nb_item, nb_user, seed=None):\n",
    "        # k embedding dimension\n",
    "        self._nb_arms = nb_item\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        self._user_feature = self._rng.uniform(0,1, nb_user*k).reshape((nb_user, k))\n",
    "        self._item_feature = self._rng.uniform(0,1, nb_item*k).reshape((nb_item, k))\n",
    "        # recommend history != state, state = user_id & list of items that have not been recommended \n",
    "        # mask matrix 0-1\n",
    "        self._recommend_mask = np.ones((nb_user, nb_item),dtype = int) # not yet recommended = 1, recommended = 0\n",
    "    \n",
    "        # compute the reward matrix, then rescale and round the reward matrix to 1,2,3,4,5\n",
    "        self._reward_matrix = self._user_feature @ self._item_feature.T \n",
    "        scale_coef = 5/np.amax(self._reward_matrix)\n",
    "        self._reward_matrix = np.around(scale_coef * self._reward_matrix, decimals = 0) \n",
    "        self._reward_matrix = self._reward_matrix.astype(int)\n",
    "      #  self._optimal_reward = np.max(self._reward)\n",
    "        \n",
    "    def step(self):\n",
    "        # return reward, next state = [user_id, [list item_id]]\n",
    "        user_to_play = self._rng.choice(self._user_feature.shape[0], 1)[0]\n",
    "        items_available = np.where(self._recommend_mask[user_to_play] == 1)[0]\n",
    "        return user_to_play, items_available\n",
    "        \n",
    "        \n",
    "    def update(self, user_id, action):\n",
    "        # update state according to user's action (choice)\n",
    "        reward = self._reward_matrix[user_id, action]\n",
    "        self._recommend_mask[user_id][action] = 0\n",
    "        next_state = [user_to_play, np.array(items_available, dtype = int)]\n",
    "        return reward, next_state\n",
    "        \n",
    "    def reset(self):\n",
    "        # first_state   \n",
    "        self._recommend_mask = np.ones((nb_user, nb_item),dtype = int)\n",
    "        return first_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    Random agent\n",
    "    \"\"\"\n",
    "    def __init__(self, _id, nb_item, seed=None):\n",
    "        self._id = _id\n",
    "        self._nb_arms = nb_item\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        \n",
    "    def act(self, state):\n",
    "        # available choices change as time evolves, state contains available choices\n",
    "        choice = self._rng.randint(len(state))\n",
    "        return state[choice]\n",
    "    \n",
    "    def getID(self):\n",
    "        return self._id\n",
    "       \n",
    "    def update(self, action, reward):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_to_play: 1, recommend items [0 1 2 3 4 5 6 7 8 9], choice 5\n",
      "reward: 1\n",
      "user_to_play: 0, recommend items [0 1 2 3 4 5 6 7 8 9], choice 5\n",
      "reward: 2\n",
      "user_to_play: 1, recommend items [0 1 2 3 4 6 7 8 9], choice 9\n",
      "reward: 2\n",
      "user_to_play: 2, recommend items [0 1 2 3 4 5 6 7 8 9], choice 8\n",
      "reward: 1\n",
      "user_to_play: 2, recommend items [0 1 2 3 4 5 6 7 9], choice 9\n",
      "reward: 1\n",
      "user_to_play: 0, recommend items [0 1 2 3 4 6 7 8 9], choice 0\n",
      "reward: 2\n",
      "user_to_play: 1, recommend items [0 1 2 3 4 6 7 8], choice 8\n",
      "reward: 2\n",
      "user_to_play: 1, recommend items [0 1 2 3 4 6 7], choice 1\n",
      "reward: 2\n",
      "user_to_play: 2, recommend items [0 1 2 3 4 5 6 7], choice 6\n",
      "reward: 0\n",
      "user_to_play: 2, recommend items [0 1 2 3 4 5 7], choice 3\n",
      "reward: 1\n",
      "user_to_play: 1, recommend items [0 2 3 4 6 7], choice 4\n",
      "reward: 1\n",
      "user_to_play: 0, recommend items [1 2 3 4 6 7 8 9], choice 4\n",
      "reward: 3\n",
      "user_to_play: 2, recommend items [0 1 2 4 5 7], choice 2\n",
      "reward: 1\n",
      "user_to_play: 0, recommend items [1 2 3 6 7 8 9], choice 6\n",
      "reward: 1\n",
      "user_to_play: 2, recommend items [0 1 4 5 7], choice 5\n",
      "reward: 1\n",
      "user_to_play: 2, recommend items [0 1 4 7], choice 0\n",
      "reward: 1\n",
      "user_to_play: 0, recommend items [1 2 3 7 8 9], choice 7\n",
      "reward: 4\n",
      "user_to_play: 2, recommend items [1 4 7], choice 7\n",
      "reward: 1\n",
      "user_to_play: 1, recommend items [0 2 3 6 7], choice 0\n",
      "reward: 1\n",
      "user_to_play: 2, recommend items [1 4], choice 4\n",
      "reward: 1\n"
     ]
    }
   ],
   "source": [
    "nb_item = 10\n",
    "nb_user = 3\n",
    "nb_iter = 20\n",
    "embedding_dimension = 2\n",
    "\n",
    "agents = [RandomAgent(i, nb_item, seed = i) for i in range(nb_user)]\n",
    "env = Environment(embedding_dimension, nb_item, nb_user, seed = 2020)\n",
    "\n",
    "for i in range(nb_iter):\n",
    "    user_to_play, items_available = env.step()\n",
    "    agent = agents[user_to_play]\n",
    "    item_chosen = agent.act(items_available)\n",
    "    print(\"user_to_play: {}, recommend items {}, choice {}\".format(user_to_play, items_available, item_chosen))\n",
    "    reward, _ = env.update(agent.getID(), item_chosen)\n",
    "    print(\"reward:\", reward)\n",
    "    agent.update(item_chosen, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
